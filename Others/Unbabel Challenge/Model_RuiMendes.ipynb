{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unbabel Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "0 - Machine translation\n",
    "1 - Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download()\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data\n",
    "\n",
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages = pd.read_table(\"data/training.txt\", header=None, names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16892, 2)\n"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "    print(messages.head())\n",
    "print(messages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#messages.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages['length'] = messages['message'].apply(len)\n",
    "if debug:\n",
    "    print(messages.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#messages['length'].plot(bins=50, kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#messages.length.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#messages[messages['length'] == 6353]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#messages.hist(column='length', by='label', bins=50,figsize=(10,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Text pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "#stopwords.words('spanish')[0:10] # Show some stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    #In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    #stops = set(stopwords.words(\"spanish\"))  \n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    #return [word.lower() for word in nopunc.split() if word.lower() not in stops]\n",
    "    return [word.lower() for word in nopunc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check to make sure its working\n",
    "processed_sentences = messages['message'].apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    messages['message'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(processed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Features creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pos_features(sentence, sentence_pos, i):\n",
    "    features = {}\n",
    "\n",
    "    #features[\"word\"] = sentence[i]\n",
    "    features[\"word-pos\"] = sentence_pos[i][1]\n",
    "\n",
    "    if i == 0:\n",
    "     #   features[\"prev-word\"] = \"<START>\"\n",
    "        features[\"prev-word-pos\"] = \"<START>\"\n",
    "    else:\n",
    "      #  features[\"prev-word\"] = sentence[i-1]\n",
    "        features[\"prev-word-pos\"] = sentence_pos[i-1][1]\n",
    "\n",
    "    if i == len(sentence) - 1:\n",
    "       # features[\"next-word\"] = \"<END>\"\n",
    "        features[\"next-word-pos\"] = \"<END>\"\n",
    "    else:\n",
    "        #features[\"next-word\"] = sentence[i+1]\n",
    "        features[\"next-word-pos\"] = sentence_pos[i+1][1]\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(processed_sentences.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "labels = messages['label']\n",
    "if debug:\n",
    "    labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(processed_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_features(sentences, labels):\n",
    "    features = []\n",
    "    for index, sentence in sentences.iteritems():\n",
    "        sentence_pos = nltk.pos_tag(sentence)\n",
    "        for i, word in enumerate(sentence):\n",
    "            #print(sentence, word, i)\n",
    "            features.append((pos_features(sentence, sentence_pos, i), labels[index], index))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = create_features(processed_sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({'word-pos': 'NN', 'prev-word-pos': '<START>', 'next-word-pos': 'FW'}, 1), ({'word-pos': 'FW', 'prev-word-pos': 'NN', 'next-word-pos': 'FW'}, 1), ({'word-pos': 'FW', 'prev-word-pos': 'FW', 'next-word-pos': 'FW'}, 1), ({'word-pos': 'FW', 'prev-word-pos': 'FW', 'next-word-pos': 'FW'}, 1), ({'word-pos': 'FW', 'prev-word-pos': 'FW', 'next-word-pos': 'FW'}, 1), ({'word-pos': 'FW', 'prev-word-pos': 'FW', 'next-word-pos': 'FW'}, 1), ({'word-pos': 'FW', 'prev-word-pos': 'FW', 'next-word-pos': 'FW'}, 1), ({'word-pos': 'FW', 'prev-word-pos': 'FW', 'next-word-pos': 'FW'}, 1), ({'word-pos': 'FW', 'prev-word-pos': 'FW', 'next-word-pos': 'FW'}, 1), ({'word-pos': 'FW', 'prev-word-pos': 'FW', 'next-word-pos': 'FW'}, 1)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if debug:\n",
    "    print(features[:10])\n",
    "print(features[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from nltk.tag import pos_tag, pos_tag_sents\n",
    "#processed_sentences_postag = pos_tag_sents(processed_sentences)\n",
    "#print(processed_sentences_postag)\n",
    "#for sentence in processed_sentences:\n",
    "    #print(list(sentence))\n",
    "#    print(pos_tag(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    " \n",
    "vec = DictVectorizer(sparse=False)\n",
    "X = vec.fit_transform([item[0] for item in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(431537, 101)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"next-word-pos=''\",\n",
       " 'next-word-pos=<END>',\n",
       " 'next-word-pos=CC',\n",
       " 'next-word-pos=CD',\n",
       " 'next-word-pos=DT',\n",
       " 'next-word-pos=EX',\n",
       " 'next-word-pos=FW',\n",
       " 'next-word-pos=IN',\n",
       " 'next-word-pos=JJ',\n",
       " 'next-word-pos=JJR',\n",
       " 'next-word-pos=JJS',\n",
       " 'next-word-pos=MD',\n",
       " 'next-word-pos=NN',\n",
       " 'next-word-pos=NNP']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(X))\n",
    " \n",
    "#print(len(X[0]))\n",
    " \n",
    "vec.get_feature_names()[1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = [item[1] for item in features]\n",
    "\n",
    "if debug:\n",
    "    print(Y[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Split dataset (train and test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test size is 20% of the entire dataset, and the training is the rest. Note the default split would have been 30/70."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, train_size=0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345229\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (345229, 101)\n",
      "X_test: (86308, 101)\n",
      "Y_train: 345229\n",
      "Y_test: 86308\n"
     ]
    }
   ],
   "source": [
    "#if debug:\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('X_test: ' + str(X_test.shape))\n",
    "print('Y_train: ' + str(len(Y_train)))\n",
    "print('Y_test: ' + str(len(Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<340202x101 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1020606 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "# now you can save it to a file\n",
    "with open('SVM_filename_1.pkl', 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "# and later you can load it\n",
    "#with open('filename.pkl', 'rb') as f:\n",
    "#    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the random forest...\")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "forest = forest.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('RandomForest_filename_1.pkl', 'wb') as f:\n",
    "    pickle.dump(forest, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from nltk.chunk import ne_chunk_sents\n",
    "#chunks = ne_chunk_sents(processed_sentences_postag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "tagger = ClassifierBasedPOSTagger(train=list(processed_sentences_postag))\n",
    "tagger.evaluate(test_sents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def process_pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.naive_bayes import MultinomialNB\n",
    "#human_detect_model = MultinomialNB().fit(messages_tfidf, messages['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try classifying our single random message and checking how we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if debug:\n",
    "#    print('predicted:', human_detect_model.predict(tfidf4)[0])\n",
    "#    print('expected:', messages.label[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(\"Training the random forest...\")\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "#forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "#forest = forest.fit(messages_tfidf, messages[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Model Evaluation\n",
    "Now we want to determine how well our model will do overall on the entire dataset. Let's beginby getting all the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_predictions = human_detect_model.predict(messages_tfidf)\n",
    "#if debug:\n",
    "#    print(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use SciKit Learn's built-in classification report, which returns precision, recall, f1-score, and a column for support (meaning how many cases supported that classification). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report\n",
    "#if debug:\n",
    "#    print(classification_report(messages['label'], all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above \"evaluation\",we evaluated accuracy on the same data we used for training. You should never actually evaluate on the same dataset you train on!\n",
    "\n",
    "\n",
    "A proper way is to split the data into a training/test set, where the model only ever sees the training data during its model fitting and parameter tuning. The test data is never used in any way. This is then our final evaluation on test data is representative of true predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all_predictions_forest = forest.predict(messages_tfidf)\n",
    "#if debug:\n",
    "#    print(all_predictions_forest)\n",
    "#    print(classification_report(messages['label'], all_predictions_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Creating a Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pipeline = Pipeline([\n",
    "#    ('bow', CountVectorizer(analyzer=text_process)),  # strings to token integer counts\n",
    "#    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
    "#    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
    "#])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pipeline.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions = pipeline.predict(msg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if debug:\n",
    "#    print(classification_report(predictions,label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('SVM_filename_1.pkl', 'rb') as fid:\n",
    "    clf2 = pickle.load(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50716040228020576"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('RandomForest_filename_1.pkl', 'rb') as fid:\n",
    "    rndforest = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rndforest.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rndforest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_blind = pd.read_csv(\"data/test_blind.txt\", sep='\\t', header=None, names=['label', 'message'],encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file = open('data/test_blind.txt', 'r')\n",
    "lines = {}\n",
    "i=0\n",
    "for line in file:\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "    lines[i] = {'label': parts[0], 'message': parts[1]}\n",
    "    i += 1\n",
    "#print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3220, 2)\n"
     ]
    }
   ],
   "source": [
    "test_blind = pd.DataFrame.from_dict(lines, orient='index')\n",
    "\n",
    "print(test_blind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_blind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3220, 2)\n"
     ]
    }
   ],
   "source": [
    "print(test_blind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_processed_sentences = test_blind['message'].apply(text_process)\n",
    "test_labels = test_blind['label'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3220,)\n"
     ]
    }
   ],
   "source": [
    "print(test_processed_sentences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_features = create_features(test_processed_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67703\n"
     ]
    }
   ],
   "source": [
    "print(len(test_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_Y = vec.transform([item[0] for item in test_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67703, 101)\n"
     ]
    }
   ],
   "source": [
    "print(test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#RANDOM FOREST\n",
    "#predicts_rnd = rndforest.predict(test_Y)\n",
    "# SVM\n",
    "predicts_rnd = clf2.predict(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67703"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicts_rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_sentences = [item[2] for item in test_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_ = zip(n_sentences, predicts_rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import collections\n",
    "sentences_test = collections.defaultdict(list)\n",
    "for i_sentence, label in predictions_:\n",
    "    sentences_test[i_sentence].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "results = []\n",
    "for k,v in sentences_test.items():\n",
    "    ones = np.count_nonzero(v)\n",
    "    total = len(v)\n",
    "    #print(\"%s - %s - %s\" % (str(k), str(ones), str(total)))\n",
    "    if (total - ones > (total/2)):\n",
    "        results.append(0)\n",
    "    else:\n",
    "        results.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for r in results:\n",
    "#    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_test = pd.DataFrame()\n",
    "results_test['message'] = test_blind['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_test['label'] =results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_test.to_csv('results_RuiMendes_svm.txt', sep='\\t', columns = ['label', 'message'], index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3220.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.023602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.151831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             label\n",
       "count  3220.000000\n",
       "mean      0.023602\n",
       "std       0.151831\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       0.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_test.to_csv('results_RuiMendes_2.txt',index=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     label                                            message\n",
      "0        _  Bougainvillea en la floración a pesar de el dí...\n",
      "1        _  El número de servicios de Internet de los últi...\n",
      "2        _  ¡ Apostamos nuestro dinero a que él hace un gr...\n",
      "3        _  Este loco Talentosos impresionista Sings conge...\n",
      "4        _  Nuestra Niñera Rompio con Nosotros con un Mens...\n",
      "5        _  La multitud se puso un poco mejor con cada art...\n",
      "6        _  No hubo alteración real de quién era yo , de l...\n",
      "7        _  Como Zuckerberg ha prometido , Oculus VR comen...\n",
      "8        _  Todo , desde el modelo básico hasta el más ava...\n",
      "9        _  Al hacer que el robot sea más como nosotros , ...\n",
      "10       _  Otra gran debilidad del programa es que no tod...\n",
      "11       _  Para celebrar el regreso de mi avatar a la pri...\n",
      "12       _  Se cree que Lea Michele ha sido una diva total...\n",
      "13       _  Cuando le expliqué a Marlo que no te gustan lo...\n",
      "14       _  Y aunque sus abs son un espectáculo para la vi...\n",
      "15       _  Si se mira directamente a la cámara , le da el...\n",
      "16       _  Estamos muy felices por el regreso de esta est...\n",
      "17       _  También había que trabajar mucho , de manera r...\n",
      "18       _  Por ejemplo , podrías pedir recibir un aviso l...\n",
      "19       _                          Veintiuna han fracasado .\n",
      "20       _   La FCC recibirá comentarios sobre la propuesta .\n",
      "21       _  Se tiene una serie de registros que contienen ...\n",
      "22       _  Es El Fin De La Dirección De Correo Electrónic...\n",
      "23       _  Este teléfono también usará Bing búsqueda por ...\n",
      "24       _   Oh , cariño , que realmente debería dejar el ...\n",
      "25       _  Al parecer , la película Avengers se ganó am c...\n",
      "26       _  Todos tenemos diminutas , cuerpos apretados , ...\n",
      "27       _  en Brooklyn , cuyos empleados tuvieron la amab...\n",
      "28       _        Obi-Wan : 50 Céntimos dedíquese a lo suyo .\n",
      "29       _  Más tarde dijo a un reportero de la televisión...\n",
      "...    ...                                                ...\n",
      "2589     _  Eso es un fuerte aumento desde los principios ...\n",
      "2590     _     The Real Drunk en amor te hará realmente LOL !\n",
      "2591     _                   ¿ Qué pasa si tomo desde abajo ?\n",
      "2592     _  Mientras salía de la reunión , una de las muje...\n",
      "2593     _  Créditos de imágenes : ilustrador Byrd de narr...\n",
      "2594     _  Ella y Harry se fuerán en ese gran viaje en la...\n",
      "2595     _  No es probable - no hay nada malo con mantener...\n",
      "2596     _  No han deseado de vez en cuando que algún tipo...\n",
      "2597     _  Pero una persona que sabía hacía prever estos ...\n",
      "2598     _  Esto incluyó una tira sobre el pánico desatado...\n",
      "2599     _  Tina Fey , con un poco de ayuda de Maya Rudolp...\n",
      "2600     _                                Oh , hola , Brené .\n",
      "2601     _  Nos gustó tanto que nos volveríamos a ver por ...\n",
      "2602     _  Bueno , sí , pero ese argumento se llevaría a ...\n",
      "2603     _  Y ni siquiera sólo de una manera \" sexytime \" ...\n",
      "2604     _  Es una gran manera de reconocer la complejidad...\n",
      "2605     _  A ) Trick un hombre convencionalmente atractiv...\n",
      "2606     _   Lo siguiente que sé suena su beau y su corazó...\n",
      "2607     _  Si alguien engañó a su novia original contigo ...\n",
      "2608     _            ( O bien , ve a YouTube y míralo allí .\n",
      "2609     _  Buenas noticias : Usted puede siempre pregunta...\n",
      "2610     _  En cualquier caso , la actriz de Mad Men llevó...\n",
      "2611     _        ¿ Tienes una cita para llevar a las bodas !\n",
      "2612     _  Si te hubieras quedado podrías haber visto esa...\n",
      "2613     _  Dice editor de Esquire , Nate Hopper : \" Anist...\n",
      "2614     _  La fundación hizo el anuncio en su blog despué...\n",
      "2615     _  Las pobres ventas de su álbum de regreso del h...\n",
      "2616     _  La sentencia del cantante incluirá los 48 días...\n",
      "2617     _      Y Jesús dijo que lo sabremos por sus frutos .\n",
      "2618     _  Hasta ahora , me comentó ella , ninguna public...\n",
      "\n",
      "[2619 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_blind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      label                                            message\n",
      "0         0  Bougainvillea en la floración a pesar de el dí...\n",
      "1         0  El número de servicios de Internet de los últi...\n",
      "2         0  ¡ Apostamos nuestro dinero a que él hace un gr...\n",
      "3         0  Este loco Talentosos impresionista Sings conge...\n",
      "4         0  Nuestra Niñera Rompio con Nosotros con un Mens...\n",
      "5         1  La multitud se puso un poco mejor con cada art...\n",
      "6         0  No hubo alteración real de quién era yo , de l...\n",
      "7         0  Como Zuckerberg ha prometido , Oculus VR comen...\n",
      "8         0  Todo , desde el modelo básico hasta el más ava...\n",
      "9         0  Al hacer que el robot sea más como nosotros , ...\n",
      "10        0  Otra gran debilidad del programa es que no tod...\n",
      "11        0  Para celebrar el regreso de mi avatar a la pri...\n",
      "12        0  Se cree que Lea Michele ha sido una diva total...\n",
      "13        0  Cuando le expliqué a Marlo que no te gustan lo...\n",
      "14        0  Y aunque sus abs son un espectáculo para la vi...\n",
      "15        0  Si se mira directamente a la cámara , le da el...\n",
      "16        0  Estamos muy felices por el regreso de esta est...\n",
      "17        0  También había que trabajar mucho , de manera r...\n",
      "18        0  Por ejemplo , podrías pedir recibir un aviso l...\n",
      "19        0                          Veintiuna han fracasado .\n",
      "20        1   La FCC recibirá comentarios sobre la propuesta .\n",
      "21        1  Se tiene una serie de registros que contienen ...\n",
      "22        0  Es El Fin De La Dirección De Correo Electrónic...\n",
      "23        0  Este teléfono también usará Bing búsqueda por ...\n",
      "24        0   Oh , cariño , que realmente debería dejar el ...\n",
      "25        1  Al parecer , la película Avengers se ganó am c...\n",
      "26        1  Todos tenemos diminutas , cuerpos apretados , ...\n",
      "27        0  en Brooklyn , cuyos empleados tuvieron la amab...\n",
      "28        0        Obi-Wan : 50 Céntimos dedíquese a lo suyo .\n",
      "29        0  Más tarde dijo a un reportero de la televisión...\n",
      "...     ...                                                ...\n",
      "2589      0  Eso es un fuerte aumento desde los principios ...\n",
      "2590      0     The Real Drunk en amor te hará realmente LOL !\n",
      "2591      0                   ¿ Qué pasa si tomo desde abajo ?\n",
      "2592      1  Mientras salía de la reunión , una de las muje...\n",
      "2593      1  Créditos de imágenes : ilustrador Byrd de narr...\n",
      "2594      0  Ella y Harry se fuerán en ese gran viaje en la...\n",
      "2595      1  No es probable - no hay nada malo con mantener...\n",
      "2596      0  No han deseado de vez en cuando que algún tipo...\n",
      "2597      0  Pero una persona que sabía hacía prever estos ...\n",
      "2598      0  Esto incluyó una tira sobre el pánico desatado...\n",
      "2599      0  Tina Fey , con un poco de ayuda de Maya Rudolp...\n",
      "2600      1                                Oh , hola , Brené .\n",
      "2601      1  Nos gustó tanto que nos volveríamos a ver por ...\n",
      "2602      0  Bueno , sí , pero ese argumento se llevaría a ...\n",
      "2603      0  Y ni siquiera sólo de una manera \" sexytime \" ...\n",
      "2604      0  Es una gran manera de reconocer la complejidad...\n",
      "2605      0  A ) Trick un hombre convencionalmente atractiv...\n",
      "2606      0   Lo siguiente que sé suena su beau y su corazó...\n",
      "2607      1  Si alguien engañó a su novia original contigo ...\n",
      "2608      0            ( O bien , ve a YouTube y míralo allí .\n",
      "2609      1  Buenas noticias : Usted puede siempre pregunta...\n",
      "2610      0  En cualquier caso , la actriz de Mad Men llevó...\n",
      "2611      0        ¿ Tienes una cita para llevar a las bodas !\n",
      "2612      0  Si te hubieras quedado podrías haber visto esa...\n",
      "2613      1  Dice editor de Esquire , Nate Hopper : \" Anist...\n",
      "2614      0  La fundación hizo el anuncio en su blog despué...\n",
      "2615      0  Las pobres ventas de su álbum de regreso del h...\n",
      "2616      0  La sentencia del cantante incluirá los 48 días...\n",
      "2617      0      Y Jesús dijo que lo sabremos por sus frutos .\n",
      "2618      0  Hasta ahora , me comentó ella , ninguna public...\n",
      "\n",
      "[2619 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "results_test_2 = test_blind\n",
    "results_test_2['label'] = results\n",
    "print(results_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_test_2.to_csv('results_RuiMendes_2.txt',index=False, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
